{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94554072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Document\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm_s = Ollama(model=\"codegemma:latest\", request_timeout=10000, temperature=0)\n",
    "llm_g = Ollama(model=\"codestral:latest\", request_timeout=10000)\n",
    "\n",
    "dependecy_folders = [\n",
    "    'Model',\n",
    "    \"Repository\"\n",
    "]\n",
    "\n",
    "base_directory = \"C:\\\\GitHub\\\\microservices-sample\\\\microservices\\\\src\\microservices\\\\CartMicroservice\"\n",
    "\n",
    "source_files = [\n",
    "    \"C:\\\\GitHub\\\\microservices-sample\\\\microservices\\\\src\\microservices\\\\CartMicroservice\\\\Controllers\\\\CartController.cs\"\n",
    "]\n",
    "\n",
    "test_target_directory = \"C:\\\\GitHub\\\\microservices-sample\\\\microservices\\\\tests\\\\CartMicroservice.UnitTests\"\n",
    "\n",
    "file_content = \"\"\n",
    "\n",
    "def strip_code(resp, start_string, end_string):\n",
    "    \"\"\"\n",
    "    Removes all content including start_string from the start,\n",
    "    and all content including end_string from the end of resp.text.\n",
    "    Returns the cleaned code as a string.\n",
    "    \"\"\"\n",
    "    text = resp.text\n",
    "    start_idx = text.find(start_string)\n",
    "    if start_idx != -1:\n",
    "        text = text[start_idx + len(start_string):]\n",
    "    end_idx = text.rfind(end_string)\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51cbe19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"method\": \"GetCartItems\",\n",
      "    \"description\": \"Should return a list of cart items for a given user\",\n",
      "    \"inputs\": { \"userId\": \"user123\" },\n",
      "    \"expected_result\": [\n",
      "      { \"catalogItemId\": \"item1\", \"name\": \"Product A\", \"price\": 10.99, \"quantity\": 2 },\n",
      "      { \"catalogItemId\": \"item2\", \"name\": \"Product B\", \"price\": 5.99, \"quantity\": 1 }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"method\": \"InsertCartItem\",\n",
      "    \"description\": \"Should add a new cart item to the user's cart\",\n",
      "    \"inputs\": {\n",
      "      \"userId\": \"user123\",\n",
      "      \"cartItem\": {\n",
      "        \"catalogItemId\": \"item3\",\n",
      "        \"name\": \"Product C\",\n",
      "        \"price\": 7.99,\n",
      "        \"quantity\": 1\n",
      "      }\n",
      "    },\n",
      "    \"expected_result\": null\n",
      "  },\n",
      "  {\n",
      "    \"method\": \"UpdateCartItem\",\n",
      "    \"description\": \"Should update an existing cart item in the user's cart\",\n",
      "    \"inputs\": {\n",
      "      \"userId\": \"user123\",\n",
      "      \"cartItem\": {\n",
      "        \"catalogItemId\": \"item1\",\n",
      "        \"name\": \"Product A\",\n",
      "        \"price\": 12.99,\n",
      "        \"quantity\": 3\n",
      "      }\n",
      "    },\n",
      "    \"expected_result\": null\n",
      "  },\n",
      "  {\n",
      "    \"method\": \"DeleteCartItem\",\n",
      "    \"description\": \"Should remove a cart item from the user's cart\",\n",
      "    \"inputs\": { \"userId\": \"user123\", \"cartItemId\": \"item2\" },\n",
      "    \"expected_result\": null\n",
      "  },\n",
      "  {\n",
      "    \"method\": \"UpdateCatalogItem\",\n",
      "    \"description\": \"Should update the name and price of a catalog item in all user carts\",\n",
      "    \"inputs\": {\n",
      "      \"catalogItemId\": \"item1\",\n",
      "      \"name\": \"Updated Product A\",\n",
      "      \"price\": 14.99\n",
      "    },\n",
      "    \"expected_result\": null\n",
      "  },\n",
      "  {\n",
      "    \"method\": \"DeleteCatalogItem\",\n",
      "    \"description\": \"Should remove a catalog item from all user carts\",\n",
      "    \"inputs\": { \"catalogItemId\": \"item2\" },\n",
      "    \"expected_result\": null\n",
      "  }\n",
      "]\n",
      "[{'method': 'GetCartItems', 'description': 'Should return a list of cart items for a given user', 'inputs': {'userId': 'user123'}, 'expected_result': [{'catalogItemId': 'item1', 'name': 'Product A', 'price': 10.99, 'quantity': 2}, {'catalogItemId': 'item2', 'name': 'Product B', 'price': 5.99, 'quantity': 1}]}, {'method': 'InsertCartItem', 'description': \"Should add a new cart item to the user's cart\", 'inputs': {'userId': 'user123', 'cartItem': {'catalogItemId': 'item3', 'name': 'Product C', 'price': 7.99, 'quantity': 1}}, 'expected_result': None}, {'method': 'UpdateCartItem', 'description': \"Should update an existing cart item in the user's cart\", 'inputs': {'userId': 'user123', 'cartItem': {'catalogItemId': 'item1', 'name': 'Product A', 'price': 12.99, 'quantity': 3}}, 'expected_result': None}, {'method': 'DeleteCartItem', 'description': \"Should remove a cart item from the user's cart\", 'inputs': {'userId': 'user123', 'cartItemId': 'item2'}, 'expected_result': None}, {'method': 'UpdateCatalogItem', 'description': 'Should update the name and price of a catalog item in all user carts', 'inputs': {'catalogItemId': 'item1', 'name': 'Updated Product A', 'price': 14.99}, 'expected_result': None}, {'method': 'DeleteCatalogItem', 'description': 'Should remove a catalog item from all user carts', 'inputs': {'catalogItemId': 'item2'}, 'expected_result': None}]\n",
      "Unit test file generated at: C:\\GitHub\\microservices-sample\\microservices\\tests\\CartMicroservice.UnitTests\\CartControllerGenAITest.cs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "#llm =  Ollama(model=\"codestral:latest\", request_timeout=10000)\n",
    "unit_test_cases = []\n",
    "\n",
    "dependecy_folders = [\n",
    "    'Model',\n",
    "    \"Repository\"\n",
    "]\n",
    "\n",
    "def get_all_dependencies(base_directory, dependecy_folders):\n",
    "    dependencies_content = []\n",
    "    for folder in dependecy_folders:\n",
    "        folder_path = os.path.join(base_directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in glob(os.path.join(folder_path, '*')):\n",
    "                if os.path.isfile(file):\n",
    "                    with open(file, 'r', encoding='utf-8') as dep_file:\n",
    "                        dependencies_content.append(dep_file.read())\n",
    "    return \"\\n----------\\n\".join(dependencies_content)\n",
    "\n",
    "\n",
    "\n",
    "## generate the unit testcases\n",
    "def generate_unit_testcases( ) :\n",
    "\n",
    "    code_dependency = get_all_dependencies(base_directory, dependecy_folders);\n",
    "    \n",
    "    # loop over source_files and print the file content \n",
    "    for file_path in source_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            file_content = f.read( )\n",
    "\n",
    "            unit_test_scenario_prompt = f\"\"\"\n",
    "            You are given the following C# source file content:\n",
    "\n",
    "            {file_content}\n",
    "\n",
    "            The following are the contents of all relevant dependency files (models, repositories, etc.):\n",
    "\n",
    "            {code_dependency}\n",
    "\n",
    "            Analyze this code and all dependencies above to generate a list of possible unit test cases for the methods in this file.\n",
    "            For each test case, make sure that all input parameters (including complex types and nested objects) are given example values for initialization, based on the definitions in the dependency files.\n",
    "\n",
    "            Return your answer strictly in the following JSON format:\n",
    "\n",
    "            [\n",
    "                {{\n",
    "                \"method\": \"<method_name>\",\n",
    "                \"description\": \"<short description of the test case>\",\n",
    "                \"inputs\": {{ \"<parameter1>\": \"<example_value>\", ... }},\n",
    "                \"expected_result\": \"<expected outcome or behavior>\"\n",
    "                }},\n",
    "                ...\n",
    "            ]\n",
    "\n",
    "            Do not include any explanations or text outside the JSON array.\n",
    "            \"\"\"\n",
    "            resp = llm_s.complete ( unit_test_scenario_prompt )\n",
    "            clean_json = strip_code( resp, \"```json\", \"```\")\n",
    "\n",
    "            print ( clean_json )\n",
    "            \n",
    "            # Extract the text from the response and load as JSON\n",
    "            unit_test_cases = json.loads(clean_json)\n",
    "        \n",
    "            print(unit_test_cases)\n",
    "\n",
    "            # Get the base filename without extension\n",
    "            input_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            # Create the test filename by adding 'Test' before the extension\n",
    "            test_filename = f\"{input_filename}GenAITest.cs\"\n",
    "            test_file_path = os.path.join(test_target_directory, test_filename)\n",
    "\n",
    "            code_dependency = get_all_dependencies(base_directory, dependecy_folders);\n",
    "\n",
    "\n",
    "            # Create a prompt to generate unit test code based on the JSON test cases\n",
    "            unit_test_code_prompt = f\"\"\"\n",
    "            You are given the following list of unit test cases in JSON format:\n",
    "\n",
    "            {json.dumps(unit_test_cases, indent=4)}\n",
    "\n",
    "            The original C# file content is:\n",
    "\n",
    "            {file_content}\n",
    "\n",
    "            The following are the contents of all relevant dependency files (models, repositories, etc.):\n",
    "\n",
    "            {code_dependency}\n",
    "\n",
    "            Analyze all dependencies above to:\n",
    "            - Initialize all required objects and mocks in the generated unit tests, no object should be left for user input for initalization\n",
    "            - Import all relevant dependencies, namespaces, and using statements so the test file compiles and runs.\n",
    "            - Use Moq to mock dependencies as needed.\n",
    "            - Use clear and descriptive test method names.\n",
    "            - Do not include explanations or comments outside the code.\n",
    "            - The test class should be named '{input_filename}Test'.\n",
    "            - The namespace should match the original file's namespace, with '.UnitTests' appended.\n",
    "\n",
    "            Return only the complete C# code for the test file.\n",
    "            \"\"\"\n",
    "\n",
    "            # Get the generated unit test code from the LLM\n",
    "            unit_test_code_resp = llm_g.complete(unit_test_code_prompt)\n",
    "            unit_test_code = unit_test_code_resp.text\n",
    "\n",
    "            # Clean the code block markers from the generated code\n",
    "            code_start = unit_test_code.find('```csharp')\n",
    "            if code_start != -1:\n",
    "                unit_test_code = unit_test_code[code_start + len('```csharp'):]\n",
    "            code_end = unit_test_code.rfind('```')\n",
    "            if code_end != -1:\n",
    "                unit_test_code = unit_test_code[:code_end]\n",
    "            unit_test_code = unit_test_code.strip()\n",
    "\n",
    "            # Save the cleaned code to the test file\n",
    "            with open(test_file_path, 'w', encoding='utf-8') as test_file:\n",
    "                test_file.write(unit_test_code)\n",
    "\n",
    "            print(f\"Unit test file generated at: {test_file_path}\")\n",
    "\n",
    "            return test_file_path\n",
    "\n",
    "ut_filepath = generate_unit_testcases( )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda27356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Determining projects to restore...\n",
      "  All projects are up-to-date for restore.\n",
      "  Middleware -> C:\\GitHub\\microservices-sample\\microservices\\src\\middlewares\\Middleware\\bin\\Debug\\net8.0\\Middleware.dll\n",
      "  CartMicroservice -> C:\\GitHub\\microservices-sample\\microservices\\src\\microservices\\CartMicroservice\\bin\\Debug\\net8.0\\CartMicroservice.dll\n",
      "C:\\GitHub\\microservices-sample\\microservices\\tests\\CartMicroservice.UnitTests\\CartControllerGenAITest.cs(63,92): error CS0664: Literal of type double cannot be implicitly converted to type 'decimal'; use an 'M' suffix to create a literal of this type [C:\\GitHub\\microservices-sample\\microservices\\tests\\CartMicroservice.UnitTests\\CartMicroservice.UnitTests.csproj]\n",
      "\n",
      "Unit test file generated at: C:\\GitHub\\microservices-sample\\microservices\\tests\\CartMicroservice.UnitTests\\CartControllerGenAITest.cs\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def run_xunit_tests(test_file_path):\n",
    "    \"\"\"\n",
    "    Runs the unit tests in the specified test file using dotnet test (xUnit).\n",
    "    Returns the output of the test run.\n",
    "    \"\"\"\n",
    "    # Find the test project directory from the test file path\n",
    "    test_project_dir = os.path.dirname(test_file_path)\n",
    "    # Run dotnet test in the test project directory\n",
    "    result = subprocess.run(\n",
    "        [\"dotnet\", \"test\", \"--filter\", f\"FullyQualifiedName~{os.path.splitext(os.path.basename(test_file_path))[0]}\"],\n",
    "        cwd=test_project_dir,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def run_code_coverage(test_file_path):\n",
    "    \"\"\"\n",
    "    Runs code coverage analysis on the specified test file using coverlet via dotnet test.\n",
    "    Returns the output of the coverage run.\n",
    "    \"\"\"\n",
    "    test_project_dir = os.path.dirname(test_file_path)\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"dotnet\", \"test\",\n",
    "            \"--collect:\\\"XPlat Code Coverage\\\"\",\n",
    "            \"--filter\", f\"FullyQualifiedName~{os.path.splitext(os.path.basename(test_file_path))[0]}\"\n",
    "        ],\n",
    "        cwd=test_project_dir,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "max_tries = 0\n",
    "\n",
    "for i in range(max_tries):\n",
    "\n",
    "    exec_result = run_xunit_tests( ut_filepath )\n",
    "\n",
    "    if exec_result.returncode != 0 :\n",
    "\n",
    "        print ( exec_result.stderr )\n",
    "        print ( exec_result.stdout )\n",
    "\n",
    "        # Create a prompt to instruct the LLM to fix the code based on the test output and errors\n",
    "        fix_prompt = f\"\"\"\n",
    "        You are given the following C# unit test file content:\n",
    "\n",
    "        {open(ut_filepath, 'r', encoding='utf-8').read()}\n",
    "\n",
    "        The following errors and output were produced when running the tests:\n",
    "\n",
    "        STDERR:\n",
    "        {exec_result.stderr}\n",
    "\n",
    "        STDOUT:\n",
    "        {exec_result.stdout}\n",
    "\n",
    "        Analyze the errors and output above. Update the test file to fix the issues so that it compiles and runs successfully.\n",
    "        - Do not remove any required tests, only fix errors.\n",
    "        - Ensure all methods have correct return types and signatures.\n",
    "        - Make sure all dependencies, namespaces, and using statements are correct.\n",
    "        - Do not include explanations or comments outside the code.\n",
    "        Return only the complete, corrected C# code for the test file.\n",
    "        \"\"\"\n",
    "        #print(fix_prompt)\n",
    "        unit_test_code_resp = llm_g.complete( fix_prompt )\n",
    "        #unit_test_code = unit_test_code_resp.text\n",
    "        #print( unit_test_code )\n",
    "        clean_code = strip_code(unit_test_code_resp, \"```csharp\", \"```\")\n",
    "\n",
    "        if os.path.exists(ut_filepath):\n",
    "            os.remove(ut_filepath)\n",
    "\n",
    "        with open(ut_filepath, 'w', encoding='utf-8') as test_file:\n",
    "                test_file.write(clean_code)\n",
    "\n",
    "        print(f\"Unit test file generated at: {ut_filepath}\")\n",
    "\n",
    "    else:\n",
    "        print ( exec_result.returncode)\n",
    "        break\n",
    "\n",
    "\n",
    "# Run code coverage after successful test execution\n",
    "coverage_output = run_code_coverage(ut_filepath)\n",
    "print(coverage_output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
